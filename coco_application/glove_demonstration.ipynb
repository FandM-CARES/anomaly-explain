{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Embeddings Demonstration\n",
    "\n",
    "This notebook is meant to demonstrate the features of Glove Embeddings to see how they can potentially be used in conjunction with the COCO dataset to numerically analyze explanations and such. The link to download the embeddings is [here](https://nlp.stanford.edu/projects/glove), and I downloaded the **6b** one with Wikipedia and all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports that may be Necessary\n",
    "\n",
    "```python\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "```\n",
    "*Note: You may need to conda install gensim*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the path to glove here\n",
    "path = r\"./dat/glove.6B.50d.txt.w2v\"\n",
    "\n",
    "#Now load the model into the variable \"glove\" (may take some time)\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use Glove\n",
    "```python\n",
    "glove[\"word\"] # Will give glove embedding vector for the word\n",
    "\n",
    "\"word\" in glove #Checks if word is in glove (acts like a dictionary\n",
    "\n",
    "glove[\"husband\"] - glove[\"man\"] + glove[\"woman\"] #Should give representation that is wife\n",
    "\n",
    "#To find most similar term to a vector:\n",
    "    \n",
    "glove.similar_by_vector(query)\n",
    "\n",
    "#More advanced way to do this\n",
    "\n",
    "glove.most_similar_cosmul(positive=['husband', 'woman'], negative=['man'])\n",
    "\n",
    "#Since they are vectors, we can find the distance using dot products\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Ideas for Symbolic Reasoning\n",
    "\n",
    "Suppose we had a bunch of **labels** from each of the outputs for the subsystems (the sem. seg & the two captions), we try to figure out how close all the different labels are across different systems.\n",
    "\n",
    "We figure out a threshold and if 2-3 of them are super close in their vectors, and another one is not, we suspect that one, generally, but here are some other general ideas:\n",
    "\n",
    "- Since each subsystem will present its own set of labels, (all the labels must be relatively close to each other) if **any of them seem abnormaly far away from the others** (maybe we can scramble to see this), then we say that it is not reasonable\n",
    "- We do the same thing across multiple ones as well, and see the distances (maybe min distances) and try to figure out at a high level who is not reasonable\n",
    "- We combine these local, and high-level checks with symbolic checks to determine overall reasonability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
