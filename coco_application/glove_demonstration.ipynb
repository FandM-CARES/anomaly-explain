{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Embeddings Demonstration\n",
    "\n",
    "This notebook is meant to demonstrate the features of Glove Embeddings to see how they can potentially be used in conjunction with the COCO dataset to numerically analyze explanations and such. The link to download the embeddings is [here](https://nlp.stanford.edu/projects/glove), and I downloaded the **6b** one with Wikipedia and all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports that may be Necessary\n",
    "\n",
    "```python\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "```\n",
    "*Note: You may need to conda install gensim*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports Cell\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the path to glove here\n",
    "path = r\"C:/Users/Vishn/Downloads/glove.6B.50d.txt.w2v\"\n",
    "\n",
    "#Now load the model into the variable \"glove\" (may take some time)\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use Glove\n",
    "```python\n",
    "glove[\"word\"] # Will give glove embedding vector for the word\n",
    "\n",
    "\"word\" in glove #Checks if word is in glove (acts like a dictionary\n",
    "\n",
    "glove[\"husband\"] - glove[\"man\"] + glove[\"woman\"] #Should give representation that is wife\n",
    "\n",
    "#To find most similar term to a vector:\n",
    "    \n",
    "glove.similar_by_vector(query)\n",
    "\n",
    "#More advanced way to do this\n",
    "\n",
    "glove.most_similar_cosmul(positive=['husband', 'woman'], negative=['man'])\n",
    "\n",
    "#Since they are vectors, we can find the distance using dot products\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wife', 0.914989709854126),\n",
       " ('husband', 0.9109995365142822),\n",
       " ('mother', 0.8880481123924255),\n",
       " ('daughter', 0.8739314079284668),\n",
       " ('grandmother', 0.831427812576294),\n",
       " ('married', 0.8221434950828552),\n",
       " ('girlfriend', 0.8158676624298096),\n",
       " ('daughters', 0.8089475631713867),\n",
       " ('sister', 0.8080084323883057),\n",
       " ('widowed', 0.8063021898269653)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.similar_by_vector(glove[\"husband\"] - glove[\"man\"] + glove[\"woman\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Ideas for Symbolic Reasoning\n",
    "\n",
    "Suppose we had a bunch of **labels** from each of the outputs for the subsystems (the sem. seg & the two captions), we try to figure out how close all the different labels are across different systems.\n",
    "\n",
    "We figure out a threshold and if 2-3 of them are super close in their vectors, and another one is not, we suspect that one, generally, but here are some other general ideas:\n",
    "\n",
    "- Since each subsystem will present its own set of labels, (all the labels must be relatively close to each other) if **any of them seem abnormaly far away from the others** (maybe we can scramble to see this), then we say that it is not reasonable\n",
    "- We do the same thing across multiple ones as well, and see the distances (maybe min distances) and try to figure out at a high level who is not reasonable\n",
    "- We combine these local, and high-level checks with symbolic checks to determine overall reasonability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Demonstration\n",
    "\n",
    "Let's take 3 different images from the CoCo Dataset where:\n",
    "\n",
    "- Two of the images will be similar, and 1 image will be different\n",
    "\n",
    "We will see if we can use the **semantic segmentation tags** + some basic distance calculations to see which images should be closest to each other:\n",
    "\n",
    "Links to the three images used:\n",
    "\n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math for Calculating Distances Between Captions\n",
    "\n",
    "Suppose we have **sem. seg** labels that identify **O** objects, assuming we are using **N** dimensional vectors, we have a (**O**x**N**) array representing our values. \n",
    "\n",
    "Now suppose we have **caption labels** that identify **K** objects, assuming we are still using **N** dimensional vectors, we have a (**K**x**N**) array representing our values. \n",
    "\n",
    "## Finding Distances\n",
    "\n",
    "When we are trying to find the distances, we need to use **multiplication** to make this easy. However, we need to remember that order matters if we were to just blindly do it:\n",
    "\n",
    "```python\n",
    "[[man], [woman], [cat]] * [[man], [woman], [cat]] = 0 #As distances between each element and itself would be zero\n",
    "\n",
    "#However\n",
    "[[cat], [woman], [man]] * [[man], [woman], [cat]] !=0 #As order is different so what will get multiplied is different\n",
    "```\n",
    "\n",
    "Therefore we will specifically use **matrix multiplication**. Therefore, the steps to find distances based on this are:\n",
    "\n",
    "\n",
    "The python code to do so is as follows (uses `numpy`):\n",
    "\n",
    "```python\n",
    "O = 7\n",
    "N = 2\n",
    "K = 6\n",
    "\n",
    "#Create two matrixes based on dimensions\n",
    "x = np.arange(14).reshape((O,N))\n",
    "y = np.arange(12).reshape((K,N))\n",
    "\n",
    "# Distances are x^2 + y^2 - 2*x*y\n",
    "dists = np.sum(x**2, axis=1)[:,np.newaxis] + np.sum(y**2, axis=1) \n",
    "dists -= 2*np.matmul(x,y.T)\n",
    "\n",
    "distances = np.sqrt(dists)\n",
    "\n",
    "#Now we find the minimum of this to represent as our distance\n",
    "#For the axis, select whichever axis is smaller (will either be zero or 1)\n",
    "\n",
    "\n",
    "np.min(distances,axis = np.argmin(distances.shape))\n",
    "\n",
    "#By doing the above, we cover the one with more objects, so that there is a greater chance of greater distances (vs not properly accounting for an object's distance to others. Though, it shouldn't really matter either way.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_a = np.array([glove[\"man\"],glove[\"woman\"],glove[\"cat\"]])\n",
    "arr_b = np.array([glove[\"man\"],glove[\"woman\"],glove[\"cat\"],glove[\"dog\"]])\n",
    "\n",
    "a = np.sum(arr_a**2,axis = 1)[:,np.newaxis]\n",
    "b = np.sum(arr_b**2,axis = 1)\n",
    "\n",
    "dists = a + b - 2*np.matmul(arr_a,arr_b.T)\n",
    "dists[dists < 1e-6] = float(0.0)\n",
    "dists = np.sqrt(dists)\n",
    "type(np.min(dists,axis = np.argmin(dists.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function based on all the computations above\n",
    "def calcuate_distances(label_set_a:list, \n",
    "                       label_set_b:list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function takes in two sets of glove embeddings vectors and returns the min distances between the two\n",
    "    \n",
    "    Parameters\n",
    "    -------------    \n",
    "    label_set_a : list \n",
    "            the first set of glove embedding vectors from one input source\n",
    "    label_set_b : list\n",
    "            the second set of glove embedding vectors from the second source\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    numpy.ndarray\n",
    "        The list of distances, where length = max(len(label_set_a),len(label_set_b))\n",
    "    \"\"\"\n",
    "    \n",
    "    #Turn both into numpy arrays\n",
    "    arr_a = np.array(label_set_a)\n",
    "    arr_b = np.array(label_set_b)\n",
    "    \n",
    "    #Square and transform as needed\n",
    "    a = np.sum(arr_a**2,axis = 1)[:,np.newaxis]\n",
    "    b = np.sum(arr_b**2,axis = 1)\n",
    "    \n",
    "    #Calculate the distances and take the square root\n",
    "    #We are also cutting off where values too small\n",
    "    dists = a + b - 2*np.matmul(arr_a,arr_b.T)\n",
    "    dists[dists < 1e-6] = float(0.0)\n",
    "    dists = np.sqrt(dists)\n",
    "    \n",
    "    #Return the minimum values across the axis with more glove embeddings\n",
    "    return np.min(dists,axis = np.argmin(dists.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function based on all the computations above\n",
    "def calcuate_distance(label_set_a:list, \n",
    "                       label_set_b:list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function takes in two sets of glove embeddings vectors and returns a single value representing the distance between the two values\n",
    "    \n",
    "    Parameters\n",
    "    -------------    \n",
    "    label_set_a : list \n",
    "            the first set of glove embedding vectors from one input source\n",
    "    label_set_b : list\n",
    "            the second set of glove embedding vectors from the second source\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    float32\n",
    "        A single value representing the distance between label_set_a and label_set_b\n",
    "    \"\"\"\n",
    "    \n",
    "    #Turn both into numpy arrays\n",
    "    arr_a = np.array(label_set_a)\n",
    "    arr_b = np.array(label_set_b)\n",
    "    \n",
    "    #Square and transform as needed\n",
    "    a = np.sum(arr_a**2,axis = 1)[:,np.newaxis]\n",
    "    b = np.sum(arr_b**2,axis = 1)\n",
    "    \n",
    "    #Calculate the distances and take the square root\n",
    "    #We are also cutting off where values too small\n",
    "    dists = a + b - 2*np.matmul(arr_a,arr_b.T)\n",
    "    dists[dists < 1e-6] = float(0.0)\n",
    "    dists = np.sqrt(dists)\n",
    "    \n",
    "    #Return the minimum values across the axis with more glove embeddings\n",
    "    return np.sum(np.min(dists,axis = np.argmin(dists.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's use our above function with labels of 2 similar images and 1 different\n",
    "\n",
    "For this test, we will simply **only use the sem.seg labels to see how well we can tell distances with that** (idea is that captions will use similar idea):\n",
    "- [Similar Image A](https://cocodataset.org/#explore?id=5253) and [Similar Image B](https://cocodataset.org/#explore?id=277614) selected on coco site by clicking *stop sign* (stop) and *traffic light* (stoplight)\n",
    "- [Different Image A](https://cocodataset.org/#explore?id=360877) selected on coco site by clicking *apple* and *chair*\n",
    "\n",
    "Let's see the 3 way comparison test for distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to manually look through available words\n",
    "# asd = list(glove.vocab.keys())\n",
    "# asd.sort()\n",
    "#print(asd[360000:370000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of representaion based on sem. seg labels\n",
    "similar_image_a = [glove[\"stop\"],glove[\"stoplight\"]]\n",
    "similar_image_b = [glove[\"stop\"],glove[\"stoplight\"],glove[\"train\"],glove[\"clock\"]]\n",
    "different_image_a = [glove[\"person\"],glove[\"bottle\"],glove[\"banana\"],glove[\"apple\"],glove[\"chair\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances between similar_image_a, similar_image_b: \n",
      "[0.        0.        3.8940194 4.6098065]\n",
      "\n",
      "\n",
      "Distances between similar_image_a, different_image_a: \n",
      "[4.951073  5.6056447 5.913838  6.0824575 5.8651605]\n",
      "\n",
      "\n",
      "Distances between similar_image_b, different_image_a: \n",
      "[4.951073  5.6056447 5.913838  5.834715  4.8754997]\n",
      "\n",
      "\n",
      "Printing the sums of each:\n",
      "8.503826\n",
      "28.418173\n",
      "27.18077\n"
     ]
    }
   ],
   "source": [
    "print(\"Distances between similar_image_a, similar_image_b: \")\n",
    "print(calcuate_distances(similar_image_a,similar_image_b))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Distances between similar_image_a, different_image_a: \")\n",
    "print(calcuate_distances(similar_image_a,different_image_a))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Distances between similar_image_b, different_image_a: \")\n",
    "print(calcuate_distances(similar_image_b,different_image_a))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Printing the sums of each:\")\n",
    "print(calcuate_distance(similar_image_a,similar_image_b))\n",
    "print(calcuate_distance(similar_image_a,different_image_a))\n",
    "print(calcuate_distance(similar_image_b,different_image_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will try the same process as above but with scrambling some of the ordering (not making it nice and uniform across the diff. inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of representaion based on sem. seg labels but with different ordering\n",
    "similar_image_a = [glove[\"stoplight\"], glove[\"stop\"]]\n",
    "similar_image_b = [glove[\"train\"], glove[\"stop\"], glove[\"stoplight\"], glove[\"clock\"]]\n",
    "different_image_a = [glove[\"bottle\"],glove[\"banana\"],glove[\"person\"],glove[\"chair\"], glove[\"apple\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances between similar_image_a, similar_image_b: \n",
      "[3.8940194 0.        0.        4.6098065]\n",
      "\n",
      "\n",
      "Distances between similar_image_a, different_image_a: \n",
      "[5.6056447 5.913838  4.951073  5.8651605 6.0824575]\n",
      "\n",
      "\n",
      "Distances between similar_image_b, different_image_a: \n",
      "[5.6056447 5.913838  4.951073  4.8754997 5.834715 ]\n",
      "\n",
      "\n",
      "Printing the sums of each:\n",
      "8.503826\n",
      "28.418175\n",
      "27.18077\n"
     ]
    }
   ],
   "source": [
    "print(\"Distances between similar_image_a, similar_image_b: \")\n",
    "print(calcuate_distances(similar_image_a,similar_image_b))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Distances between similar_image_a, different_image_a: \")\n",
    "print(calcuate_distances(similar_image_a,different_image_a))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Distances between similar_image_b, different_image_a: \")\n",
    "print(calcuate_distances(similar_image_b,different_image_a))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Printing the sums of each:\")\n",
    "print(calcuate_distance(similar_image_a,similar_image_b))\n",
    "print(calcuate_distance(similar_image_a,different_image_a))\n",
    "print(calcuate_distance(similar_image_b,different_image_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "As you can see from the cell above, the two *similar images* were close to each other, **and had similar distances to the different image** which can be really useful for **outlier analysis**. \n",
    "\n",
    "Another great thing that you can see is that *independent of the order of the different label input* **the outputs for the overall distances were identical** (and the vectors just in a different ordering, same values)\n",
    "\n",
    "- We can also try this with higher dimensional (this was just with 50D vectors) so higher dimensional might lead to even tighter distances (closer things are closer, farther things are farther)\n",
    "- We can also try this process and calculate distances **within 1 single image label set to find outlier labels** (t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
